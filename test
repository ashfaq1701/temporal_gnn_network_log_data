ms2420@entropy:~/temporal_gnn_network_log_data_2$ ./run_workload_preds.sh 25
>>>>>>>start training : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0122819
	speed: 0.0349s/iter; left time: 623.8880s
	iters: 200, epoch: 1 | loss: 0.0087363
	speed: 0.0180s/iter; left time: 319.8032s
	iters: 300, epoch: 1 | loss: 0.0148014
	speed: 0.0191s/iter; left time: 337.4159s
Epoch: 1 cost time: 7.912314176559448
Epoch: 1, Steps: 359 | Train Loss: 0.0391594 Vali Loss: 0.0192592 Test Loss: 0.0191512
Validation loss decreased (inf --> 0.019259).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0059934
	speed: 0.0489s/iter; left time: 854.7334s
	iters: 200, epoch: 2 | loss: 0.0054799
	speed: 0.0193s/iter; left time: 335.6292s
	iters: 300, epoch: 2 | loss: 0.0147471
	speed: 0.0189s/iter; left time: 327.6356s
Epoch: 2 cost time: 6.910425424575806
Epoch: 2, Steps: 359 | Train Loss: 0.0075456 Vali Loss: 0.0197892 Test Loss: 0.0196393
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0024064
	speed: 0.0502s/iter; left time: 859.6917s
	iters: 200, epoch: 3 | loss: 0.0026339
	speed: 0.0195s/iter; left time: 332.5372s
	iters: 300, epoch: 3 | loss: 0.0032351
	speed: 0.0187s/iter; left time: 316.0927s
Epoch: 3 cost time: 7.201117753982544
Epoch: 3, Steps: 359 | Train Loss: 0.0043574 Vali Loss: 0.0141611 Test Loss: 0.0143693
Validation loss decreased (0.019259 --> 0.014161).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0027954
	speed: 0.0495s/iter; left time: 830.2992s
	iters: 200, epoch: 4 | loss: 0.0032014
	speed: 0.0187s/iter; left time: 312.5308s
	iters: 300, epoch: 4 | loss: 0.0027021
	speed: 0.0190s/iter; left time: 314.4348s
Epoch: 4 cost time: 6.56692910194397
Epoch: 4, Steps: 359 | Train Loss: 0.0033267 Vali Loss: 0.0140942 Test Loss: 0.0140086
Validation loss decreased (0.014161 --> 0.014094).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0025794
	speed: 0.0512s/iter; left time: 839.8466s
	iters: 200, epoch: 5 | loss: 0.0020034
	speed: 0.0174s/iter; left time: 284.1340s
	iters: 300, epoch: 5 | loss: 0.0023953
	speed: 0.0186s/iter; left time: 301.3797s
Epoch: 5 cost time: 6.676016807556152
Epoch: 5, Steps: 359 | Train Loss: 0.0029463 Vali Loss: 0.0137342 Test Loss: 0.0136444
Validation loss decreased (0.014094 --> 0.013734).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0026445
	speed: 0.0489s/iter; left time: 784.7884s
	iters: 200, epoch: 6 | loss: 0.0022990
	speed: 0.0186s/iter; left time: 297.4184s
	iters: 300, epoch: 6 | loss: 0.0107662
	speed: 0.0188s/iter; left time: 297.9628s
Epoch: 6 cost time: 6.742483615875244
Epoch: 6, Steps: 359 | Train Loss: 0.0027824 Vali Loss: 0.0138989 Test Loss: 0.0138038
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0019432
	speed: 0.0458s/iter; left time: 718.2720s
	iters: 200, epoch: 7 | loss: 0.0023856
	speed: 0.0178s/iter; left time: 278.2683s
	iters: 300, epoch: 7 | loss: 0.0022474
	speed: 0.0183s/iter; left time: 283.5651s
Epoch: 7 cost time: 6.58006477355957
Epoch: 7, Steps: 359 | Train Loss: 0.0026799 Vali Loss: 0.0141481 Test Loss: 0.0140587
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0015886
	speed: 0.0473s/iter; left time: 725.9156s
	iters: 200, epoch: 8 | loss: 0.0024211
	speed: 0.0188s/iter; left time: 286.7743s
	iters: 300, epoch: 8 | loss: 0.0024430
	speed: 0.0190s/iter; left time: 287.8175s
Epoch: 8 cost time: 6.942887544631958
Epoch: 8, Steps: 359 | Train Loss: 0.0025836 Vali Loss: 0.0136641 Test Loss: 0.0135622
Validation loss decreased (0.013734 --> 0.013664).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0029727
	speed: 0.0505s/iter; left time: 757.1767s
	iters: 200, epoch: 9 | loss: 0.0027086
	speed: 0.0166s/iter; left time: 246.9954s
	iters: 300, epoch: 9 | loss: 0.0028090
	speed: 0.0169s/iter; left time: 249.9078s
Epoch: 9 cost time: 6.111843109130859
Epoch: 9, Steps: 359 | Train Loss: 0.0025630 Vali Loss: 0.0133113 Test Loss: 0.0132147
Validation loss decreased (0.013664 --> 0.013311).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0025372
	speed: 0.0488s/iter; left time: 713.4627s
	iters: 200, epoch: 10 | loss: 0.0026477
	speed: 0.0172s/iter; left time: 249.1570s
	iters: 300, epoch: 10 | loss: 0.0024503
	speed: 0.0170s/iter; left time: 245.7857s
Epoch: 10 cost time: 6.1482253074646
Epoch: 10, Steps: 359 | Train Loss: 0.0025440 Vali Loss: 0.0135576 Test Loss: 0.0134646
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0016599
	speed: 0.0431s/iter; left time: 614.6700s
	iters: 200, epoch: 11 | loss: 0.0021848
	speed: 0.0163s/iter; left time: 230.4466s
	iters: 300, epoch: 11 | loss: 0.0022929
	speed: 0.0164s/iter; left time: 230.0766s
Epoch: 11 cost time: 5.926179647445679
Epoch: 11, Steps: 359 | Train Loss: 0.0025865 Vali Loss: 0.0135557 Test Loss: 0.0134784
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.0026183
	speed: 0.0431s/iter; left time: 599.5993s
	iters: 200, epoch: 12 | loss: 0.0096241
	speed: 0.0168s/iter; left time: 231.3759s
	iters: 300, epoch: 12 | loss: 0.0029128
	speed: 0.0170s/iter; left time: 232.5216s
Epoch: 12 cost time: 6.13749361038208
Epoch: 12, Steps: 359 | Train Loss: 0.0025174 Vali Loss: 0.0134233 Test Loss: 0.0133274
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.0018763
	speed: 0.0437s/iter; left time: 592.2698s
	iters: 200, epoch: 13 | loss: 0.0019244
	speed: 0.0171s/iter; left time: 229.8020s
	iters: 300, epoch: 13 | loss: 0.0019684
	speed: 0.0172s/iter; left time: 228.8811s
Epoch: 13 cost time: 6.148457765579224
Epoch: 13, Steps: 359 | Train Loss: 0.0025093 Vali Loss: 0.0135270 Test Loss: 0.0134294
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
	iters: 100, epoch: 14 | loss: 0.0016756
	speed: 0.0421s/iter; left time: 555.6599s
	iters: 200, epoch: 14 | loss: 0.0026144
	speed: 0.0166s/iter; left time: 217.0354s
	iters: 300, epoch: 14 | loss: 0.0017051
	speed: 0.0171s/iter; left time: 222.2534s
Epoch: 14 cost time: 6.002326965332031
Epoch: 14, Steps: 359 | Train Loss: 0.0024984 Vali Loss: 0.0134676 Test Loss: 0.0134335
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (134, 32, 3, 1) (134, 32, 3, 1)
test shape: (4288, 3, 1) (4288, 3, 1)
mse:0.013214717619121075, mae:0.03624605014920235
>>>>>>>predicting : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0095751
	speed: 0.0353s/iter; left time: 630.9127s
	iters: 200, epoch: 1 | loss: 0.0109734
	speed: 0.0190s/iter; left time: 336.9044s
	iters: 300, epoch: 1 | loss: 0.0063164
	speed: 0.0188s/iter; left time: 331.6058s
Epoch: 1 cost time: 7.988337278366089
Epoch: 1, Steps: 359 | Train Loss: 0.0828415 Vali Loss: 0.0213089 Test Loss: 0.0212405
Validation loss decreased (inf --> 0.021309).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0081170
	speed: 0.0485s/iter; left time: 849.1548s
	iters: 200, epoch: 2 | loss: 0.0062169
	speed: 0.0183s/iter; left time: 318.4884s
	iters: 300, epoch: 2 | loss: 0.0055424
	speed: 0.0185s/iter; left time: 320.6372s
Epoch: 2 cost time: 6.734543800354004
Epoch: 2, Steps: 359 | Train Loss: 0.0081420 Vali Loss: 0.0195124 Test Loss: 0.0198012
Validation loss decreased (0.021309 --> 0.019512).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0051855
	speed: 0.0507s/iter; left time: 869.4105s
	iters: 200, epoch: 3 | loss: 0.0041410
	speed: 0.0192s/iter; left time: 326.4675s
	iters: 300, epoch: 3 | loss: 0.0053905
	speed: 0.0187s/iter; left time: 315.8504s
Epoch: 3 cost time: 6.739182233810425
Epoch: 3, Steps: 359 | Train Loss: 0.0052036 Vali Loss: 0.0181243 Test Loss: 0.0180412
Validation loss decreased (0.019512 --> 0.018124).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0027685
	speed: 0.0486s/iter; left time: 815.5987s
	iters: 200, epoch: 4 | loss: 0.0033880
	speed: 0.0189s/iter; left time: 315.6883s
	iters: 300, epoch: 4 | loss: 0.0139053
	speed: 0.0188s/iter; left time: 311.6289s
Epoch: 4 cost time: 6.809211015701294
Epoch: 4, Steps: 359 | Train Loss: 0.0040580 Vali Loss: 0.0145217 Test Loss: 0.0144154
Validation loss decreased (0.018124 --> 0.014522).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0029139
	speed: 0.0485s/iter; left time: 796.4506s
	iters: 200, epoch: 5 | loss: 0.0029996
	speed: 0.0185s/iter; left time: 302.3612s
	iters: 300, epoch: 5 | loss: 0.0030781
	speed: 0.0190s/iter; left time: 308.1614s
Epoch: 5 cost time: 6.7462158203125
Epoch: 5, Steps: 359 | Train Loss: 0.0036417 Vali Loss: 0.0145882 Test Loss: 0.0144754
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0022537
	speed: 0.0475s/iter; left time: 763.1364s
	iters: 200, epoch: 6 | loss: 0.0109339
	speed: 0.0190s/iter; left time: 303.7330s
	iters: 300, epoch: 6 | loss: 0.0038285
	speed: 0.0186s/iter; left time: 295.6732s
Epoch: 6 cost time: 6.854862213134766
Epoch: 6, Steps: 359 | Train Loss: 0.0033239 Vali Loss: 0.0140308 Test Loss: 0.0139027
Validation loss decreased (0.014522 --> 0.014031).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0028630
	speed: 0.0527s/iter; left time: 827.3263s
	iters: 200, epoch: 7 | loss: 0.0027357
	speed: 0.0187s/iter; left time: 291.9786s
	iters: 300, epoch: 7 | loss: 0.0022303
	speed: 0.0187s/iter; left time: 290.5291s
Epoch: 7 cost time: 6.9640583992004395
Epoch: 7, Steps: 359 | Train Loss: 0.0032262 Vali Loss: 0.0140385 Test Loss: 0.0139135
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0022949
	speed: 0.0473s/iter; left time: 725.0553s
	iters: 200, epoch: 8 | loss: 0.0022758
	speed: 0.0199s/iter; left time: 303.5119s
	iters: 300, epoch: 8 | loss: 0.0026689
	speed: 0.0199s/iter; left time: 301.0787s
Epoch: 8 cost time: 7.118115186691284
Epoch: 8, Steps: 359 | Train Loss: 0.0031377 Vali Loss: 0.0141794 Test Loss: 0.0140588
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0088727
	speed: 0.0471s/iter; left time: 705.9241s
	iters: 200, epoch: 9 | loss: 0.0035913
	speed: 0.0192s/iter; left time: 285.2293s
	iters: 300, epoch: 9 | loss: 0.0034372
	speed: 0.0193s/iter; left time: 285.6212s
Epoch: 9 cost time: 7.022742033004761
Epoch: 9, Steps: 359 | Train Loss: 0.0030681 Vali Loss: 0.0141296 Test Loss: 0.0139992
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0031102
	speed: 0.0479s/iter; left time: 700.8156s
	iters: 200, epoch: 10 | loss: 0.0073344
	speed: 0.0182s/iter; left time: 263.6885s
	iters: 300, epoch: 10 | loss: 0.0025899
	speed: 0.0178s/iter; left time: 257.0924s
Epoch: 10 cost time: 6.796607494354248
Epoch: 10, Steps: 359 | Train Loss: 0.0030672 Vali Loss: 0.0140659 Test Loss: 0.0139552
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0092465
	speed: 0.0468s/iter; left time: 667.8904s
	iters: 200, epoch: 11 | loss: 0.0033436
	speed: 0.0194s/iter; left time: 275.3484s
	iters: 300, epoch: 11 | loss: 0.0027430
	speed: 0.0193s/iter; left time: 270.7944s
Epoch: 11 cost time: 6.96219539642334
Epoch: 11, Steps: 359 | Train Loss: 0.0030077 Vali Loss: 0.0142745 Test Loss: 0.0141587
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (134, 32, 3, 1) (134, 32, 3, 1)
test shape: (4288, 3, 1) (4288, 3, 1)
mse:0.013902700506150723, mae:0.03864109143614769
>>>>>>>predicting : informer_sl12_ll6_pl3_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0292611
	speed: 0.0424s/iter; left time: 754.7074s
	iters: 200, epoch: 1 | loss: 0.0596940
	speed: 0.0255s/iter; left time: 451.4295s
	iters: 300, epoch: 1 | loss: 0.0313102
	speed: 0.0252s/iter; left time: 444.3619s
Epoch: 1 cost time: 10.353069543838501
Epoch: 1, Steps: 358 | Train Loss: 0.0783402 Vali Loss: 0.0952454 Test Loss: 0.0956656
Validation loss decreased (inf --> 0.095245).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0125019
	speed: 0.0657s/iter; left time: 1146.5350s
	iters: 200, epoch: 2 | loss: 0.0159071
	speed: 0.0253s/iter; left time: 439.5910s
	iters: 300, epoch: 2 | loss: 0.0112055
	speed: 0.0252s/iter; left time: 433.9106s
Epoch: 2 cost time: 9.033845901489258
Epoch: 2, Steps: 358 | Train Loss: 0.0188023 Vali Loss: 0.0728746 Test Loss: 0.0723487
Validation loss decreased (0.095245 --> 0.072875).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0063891
	speed: 0.0670s/iter; left time: 1144.1107s
	iters: 200, epoch: 3 | loss: 0.0077162
	speed: 0.0252s/iter; left time: 428.2359s
	iters: 300, epoch: 3 | loss: 0.0052150
	speed: 0.0252s/iter; left time: 426.2457s
Epoch: 3 cost time: 9.048229455947876
Epoch: 3, Steps: 358 | Train Loss: 0.0095340 Vali Loss: 0.0602875 Test Loss: 0.0604032
Validation loss decreased (0.072875 --> 0.060288).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0061744
	speed: 0.0700s/iter; left time: 1171.0493s
	iters: 200, epoch: 4 | loss: 0.0123278
	speed: 0.0252s/iter; left time: 419.7829s
	iters: 300, epoch: 4 | loss: 0.0043107
	speed: 0.0252s/iter; left time: 416.9351s
Epoch: 4 cost time: 9.093451976776123
Epoch: 4, Steps: 358 | Train Loss: 0.0075561 Vali Loss: 0.0590623 Test Loss: 0.0590526
Validation loss decreased (0.060288 --> 0.059062).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0059368
	speed: 0.0691s/iter; left time: 1131.0058s
	iters: 200, epoch: 5 | loss: 0.0045979
	speed: 0.0253s/iter; left time: 411.6177s
	iters: 300, epoch: 5 | loss: 0.0032095
	speed: 0.0255s/iter; left time: 412.7293s
Epoch: 5 cost time: 9.097822666168213
Epoch: 5, Steps: 358 | Train Loss: 0.0064702 Vali Loss: 0.0578211 Test Loss: 0.0578636
Validation loss decreased (0.059062 --> 0.057821).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0048190
	speed: 0.0702s/iter; left time: 1123.9747s
	iters: 200, epoch: 6 | loss: 0.0080482
	speed: 0.0256s/iter; left time: 407.2397s
	iters: 300, epoch: 6 | loss: 0.0043551
	speed: 0.0256s/iter; left time: 404.4786s
Epoch: 6 cost time: 9.198105096817017
Epoch: 6, Steps: 358 | Train Loss: 0.0060284 Vali Loss: 0.0580835 Test Loss: 0.0581381
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0040020
	speed: 0.0650s/iter; left time: 1016.8989s
	iters: 200, epoch: 7 | loss: 0.0050741
	speed: 0.0257s/iter; left time: 399.5119s
	iters: 300, epoch: 7 | loss: 0.0053364
	speed: 0.0257s/iter; left time: 397.3676s
Epoch: 7 cost time: 9.232277870178223
Epoch: 7, Steps: 358 | Train Loss: 0.0057228 Vali Loss: 0.0580157 Test Loss: 0.0578659
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0199486
	speed: 0.0647s/iter; left time: 989.4580s
	iters: 200, epoch: 8 | loss: 0.0036869
	speed: 0.0256s/iter; left time: 389.4972s
	iters: 300, epoch: 8 | loss: 0.0053634
	speed: 0.0255s/iter; left time: 384.3995s
Epoch: 8 cost time: 9.151411294937134
Epoch: 8, Steps: 358 | Train Loss: 0.0055394 Vali Loss: 0.0577007 Test Loss: 0.0580821
Validation loss decreased (0.057821 --> 0.057701).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0056911
	speed: 0.0706s/iter; left time: 1054.8461s
	iters: 200, epoch: 9 | loss: 0.0048556
	speed: 0.0271s/iter; left time: 402.1221s
	iters: 300, epoch: 9 | loss: 0.0040554
	speed: 0.0258s/iter; left time: 379.8267s
Epoch: 9 cost time: 9.50931167602539
Epoch: 9, Steps: 358 | Train Loss: 0.0054948 Vali Loss: 0.0577194 Test Loss: 0.0575271
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0045226
	speed: 0.0652s/iter; left time: 950.0260s
	iters: 200, epoch: 10 | loss: 0.0035629
	speed: 0.0258s/iter; left time: 373.7476s
	iters: 300, epoch: 10 | loss: 0.0050991
	speed: 0.0258s/iter; left time: 371.0679s
Epoch: 10 cost time: 9.261976957321167
Epoch: 10, Steps: 358 | Train Loss: 0.0054221 Vali Loss: 0.0579630 Test Loss: 0.0582642
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0139253
	speed: 0.0662s/iter; left time: 941.4584s
	iters: 200, epoch: 11 | loss: 0.0044084
	speed: 0.0260s/iter; left time: 366.8224s
	iters: 300, epoch: 11 | loss: 0.0066423
	speed: 0.0258s/iter; left time: 361.0594s
Epoch: 11 cost time: 9.368370294570923
Epoch: 11, Steps: 358 | Train Loss: 0.0054355 Vali Loss: 0.0580313 Test Loss: 0.0575528
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.0038787
	speed: 0.0650s/iter; left time: 900.7552s
	iters: 200, epoch: 12 | loss: 0.0053422
	speed: 0.0255s/iter; left time: 350.7512s
	iters: 300, epoch: 12 | loss: 0.0054232
	speed: 0.0256s/iter; left time: 349.4850s
Epoch: 12 cost time: 9.16521668434143
Epoch: 12, Steps: 358 | Train Loss: 0.0054089 Vali Loss: 0.0581174 Test Loss: 0.0579395
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.0040598
	speed: 0.0654s/iter; left time: 883.5374s
	iters: 200, epoch: 13 | loss: 0.0201251
	speed: 0.0257s/iter; left time: 344.6759s
	iters: 300, epoch: 13 | loss: 0.0042950
	speed: 0.0258s/iter; left time: 342.7983s
Epoch: 13 cost time: 9.264735221862793
Epoch: 13, Steps: 358 | Train Loss: 0.0053934 Vali Loss: 0.0577568 Test Loss: 0.0575587
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (133, 32, 12, 1) (133, 32, 12, 1)
test shape: (4256, 12, 1) (4256, 12, 1)
mse:0.057716455310583115, mae:0.07687219977378845
>>>>>>>predicting : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0220390
	speed: 0.0418s/iter; left time: 744.9169s
	iters: 200, epoch: 1 | loss: 0.0227413
	speed: 0.0254s/iter; left time: 448.9686s
	iters: 300, epoch: 1 | loss: 0.0133881
	speed: 0.0253s/iter; left time: 445.8970s
Epoch: 1 cost time: 10.31433367729187
Epoch: 1, Steps: 358 | Train Loss: 0.1110199 Vali Loss: 0.0733967 Test Loss: 0.0728724
Validation loss decreased (inf --> 0.073397).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0141192
	speed: 0.0659s/iter; left time: 1148.9510s
	iters: 200, epoch: 2 | loss: 0.0099823
	speed: 0.0252s/iter; left time: 437.1620s
	iters: 300, epoch: 2 | loss: 0.0128255
	speed: 0.0252s/iter; left time: 435.1131s
Epoch: 2 cost time: 9.07336163520813
Epoch: 2, Steps: 358 | Train Loss: 0.0155572 Vali Loss: 0.0759295 Test Loss: 0.0753732
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0059198
	speed: 0.0642s/iter; left time: 1096.2998s
	iters: 200, epoch: 3 | loss: 0.0055876
	speed: 0.0255s/iter; left time: 432.4104s
	iters: 300, epoch: 3 | loss: 0.0157292
	speed: 0.0255s/iter; left time: 430.8277s
Epoch: 3 cost time: 9.136133432388306
Epoch: 3, Steps: 358 | Train Loss: 0.0094754 Vali Loss: 0.0735689 Test Loss: 0.0739376
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0064129
	speed: 0.0642s/iter; left time: 1073.5554s
	iters: 200, epoch: 4 | loss: 0.0168936
	speed: 0.0254s/iter; left time: 423.1413s
	iters: 300, epoch: 4 | loss: 0.0041231
	speed: 0.0254s/iter; left time: 420.2145s
Epoch: 4 cost time: 9.098771810531616
Epoch: 4, Steps: 358 | Train Loss: 0.0077439 Vali Loss: 0.0736727 Test Loss: 0.0739613
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0056684
	speed: 0.0637s/iter; left time: 1042.5131s
	iters: 200, epoch: 5 | loss: 0.0296723
	speed: 0.0256s/iter; left time: 416.0864s
	iters: 300, epoch: 5 | loss: 0.0044284
	speed: 0.0256s/iter; left time: 413.1910s
Epoch: 5 cost time: 9.124544620513916
Epoch: 5, Steps: 358 | Train Loss: 0.0066562 Vali Loss: 0.0734467 Test Loss: 0.0738090
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0055162
	speed: 0.0640s/iter; left time: 1024.9326s
	iters: 200, epoch: 6 | loss: 0.0039046
	speed: 0.0254s/iter; left time: 404.3654s
	iters: 300, epoch: 6 | loss: 0.0039618
	speed: 0.0256s/iter; left time: 404.2918s
Epoch: 6 cost time: 9.133036851882935
Epoch: 6, Steps: 358 | Train Loss: 0.0064146 Vali Loss: 0.0747481 Test Loss: 0.0747834
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (133, 32, 12, 1) (133, 32, 12, 1)
test shape: (4256, 12, 1) (4256, 12, 1)
mse:0.07353243231773376, mae:0.09161633998155594
>>>>>>>predicting : informer_sl48_ll24_pl12_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.1032785
	speed: 0.0561s/iter; left time: 993.3886s
	iters: 200, epoch: 1 | loss: 0.0538786
	speed: 0.0396s/iter; left time: 697.0010s
	iters: 300, epoch: 1 | loss: 0.0319143
	speed: 0.0397s/iter; left time: 694.4959s
Epoch: 1 cost time: 15.36077880859375
Epoch: 1, Steps: 356 | Train Loss: 0.0798251 Vali Loss: 0.1038800 Test Loss: 0.1042572
Validation loss decreased (inf --> 0.103880).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0167330
	speed: 0.1029s/iter; left time: 1785.1432s
	iters: 200, epoch: 2 | loss: 0.0187240
	speed: 0.0400s/iter; left time: 689.7899s
	iters: 300, epoch: 2 | loss: 0.0147384
	speed: 0.0397s/iter; left time: 681.2052s
Epoch: 2 cost time: 14.188717603683472
Epoch: 2, Steps: 356 | Train Loss: 0.0235332 Vali Loss: 0.1012861 Test Loss: 0.1004360
Validation loss decreased (0.103880 --> 0.101286).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0082056
	speed: 0.1069s/iter; left time: 1816.0104s
	iters: 200, epoch: 3 | loss: 0.0100694
	speed: 0.0400s/iter; left time: 675.7633s
	iters: 300, epoch: 3 | loss: 0.0215533
	speed: 0.0400s/iter; left time: 671.3328s
Epoch: 3 cost time: 14.230802059173584
Epoch: 3, Steps: 356 | Train Loss: 0.0141727 Vali Loss: 0.0969239 Test Loss: 0.0967495
Validation loss decreased (0.101286 --> 0.096924).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0066497
	speed: 0.1068s/iter; left time: 1775.8673s
	iters: 200, epoch: 4 | loss: 0.0114911
	speed: 0.0399s/iter; left time: 659.8790s
	iters: 300, epoch: 4 | loss: 0.0141716
	speed: 0.0399s/iter; left time: 656.3311s
Epoch: 4 cost time: 14.21043848991394
Epoch: 4, Steps: 356 | Train Loss: 0.0109586 Vali Loss: 0.0967309 Test Loss: 0.0961492
Validation loss decreased (0.096924 --> 0.096731).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0050797
	speed: 0.1046s/iter; left time: 1703.2546s
	iters: 200, epoch: 5 | loss: 0.0056040
	speed: 0.0400s/iter; left time: 647.4278s
	iters: 300, epoch: 5 | loss: 0.0059623
	speed: 0.0400s/iter; left time: 642.8632s
Epoch: 5 cost time: 14.255708694458008
Epoch: 5, Steps: 356 | Train Loss: 0.0088779 Vali Loss: 0.0937868 Test Loss: 0.0937616
Validation loss decreased (0.096731 --> 0.093787).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0066967
	speed: 0.1073s/iter; left time: 1708.6587s
	iters: 200, epoch: 6 | loss: 0.0072310
	speed: 0.0401s/iter; left time: 634.3740s
	iters: 300, epoch: 6 | loss: 0.0068719
	speed: 0.0401s/iter; left time: 630.2952s
Epoch: 6 cost time: 14.273608207702637
Epoch: 6, Steps: 356 | Train Loss: 0.0080720 Vali Loss: 0.0942538 Test Loss: 0.0943825
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0152815
	speed: 0.1019s/iter; left time: 1585.8380s
	iters: 200, epoch: 7 | loss: 0.0048497
	speed: 0.0402s/iter; left time: 621.5250s
	iters: 300, epoch: 7 | loss: 0.0104755
	speed: 0.0402s/iter; left time: 618.3328s
Epoch: 7 cost time: 14.30213713645935
Epoch: 7, Steps: 356 | Train Loss: 0.0076206 Vali Loss: 0.0928891 Test Loss: 0.0927826
Validation loss decreased (0.093787 --> 0.092889).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0046365
	speed: 0.1073s/iter; left time: 1631.9460s
	iters: 200, epoch: 8 | loss: 0.0046007
	speed: 0.0402s/iter; left time: 606.6322s
	iters: 300, epoch: 8 | loss: 0.0062008
	speed: 0.0402s/iter; left time: 602.7118s
Epoch: 8 cost time: 14.293396472930908
Epoch: 8, Steps: 356 | Train Loss: 0.0072717 Vali Loss: 0.0917245 Test Loss: 0.0918753
Validation loss decreased (0.092889 --> 0.091724).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0081105
	speed: 0.1072s/iter; left time: 1591.5119s
	iters: 200, epoch: 9 | loss: 0.0060879
	speed: 0.0401s/iter; left time: 591.7767s
	iters: 300, epoch: 9 | loss: 0.0226376
	speed: 0.0400s/iter; left time: 585.6915s
Epoch: 9 cost time: 14.260499954223633
Epoch: 9, Steps: 356 | Train Loss: 0.0071625 Vali Loss: 0.0919061 Test Loss: 0.0921929
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0112543
	speed: 0.1016s/iter; left time: 1472.6634s
	iters: 200, epoch: 10 | loss: 0.0152001
	speed: 0.0400s/iter; left time: 575.5089s
	iters: 300, epoch: 10 | loss: 0.0037134
	speed: 0.0400s/iter; left time: 571.6581s
Epoch: 10 cost time: 14.219172716140747
Epoch: 10, Steps: 356 | Train Loss: 0.0072450 Vali Loss: 0.0915613 Test Loss: 0.0915941
Validation loss decreased (0.091724 --> 0.091561).  Saving model ...
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0063176
	speed: 0.1068s/iter; left time: 1509.7876s
	iters: 200, epoch: 11 | loss: 0.0154703
	speed: 0.0400s/iter; left time: 561.5029s
	iters: 300, epoch: 11 | loss: 0.0045476
	speed: 0.0400s/iter; left time: 557.4801s
Epoch: 11 cost time: 14.224171876907349
Epoch: 11, Steps: 356 | Train Loss: 0.0070452 Vali Loss: 0.0912100 Test Loss: 0.0909846
Validation loss decreased (0.091561 --> 0.091210).  Saving model ...
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.0049088
	speed: 0.1058s/iter; left time: 1458.5583s
	iters: 200, epoch: 12 | loss: 0.0042183
	speed: 0.0399s/iter; left time: 546.6375s
	iters: 300, epoch: 12 | loss: 0.0039432
	speed: 0.0400s/iter; left time: 543.0618s
Epoch: 12 cost time: 14.203069686889648
Epoch: 12, Steps: 356 | Train Loss: 0.0071314 Vali Loss: 0.0919621 Test Loss: 0.0922417
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.0071916
	speed: 0.1014s/iter; left time: 1361.4272s
	iters: 200, epoch: 13 | loss: 0.0081764
	speed: 0.0400s/iter; left time: 533.0687s
	iters: 300, epoch: 13 | loss: 0.0049714
	speed: 0.0400s/iter; left time: 528.9089s
Epoch: 13 cost time: 14.223504781723022
Epoch: 13, Steps: 356 | Train Loss: 0.0070179 Vali Loss: 0.0912729 Test Loss: 0.0916019
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.44140625e-08
	iters: 100, epoch: 14 | loss: 0.0118306
	speed: 0.1016s/iter; left time: 1327.8028s
	iters: 200, epoch: 14 | loss: 0.0051560
	speed: 0.0400s/iter; left time: 518.7962s
	iters: 300, epoch: 14 | loss: 0.0090902
	speed: 0.0400s/iter; left time: 514.7565s
Epoch: 14 cost time: 14.229636430740356
Epoch: 14, Steps: 356 | Train Loss: 0.0069038 Vali Loss: 0.0917143 Test Loss: 0.0924761
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.220703125e-08
	iters: 100, epoch: 15 | loss: 0.0049710
	speed: 0.1016s/iter; left time: 1291.6466s
	iters: 200, epoch: 15 | loss: 0.0049803
	speed: 0.0400s/iter; left time: 504.5219s
	iters: 300, epoch: 15 | loss: 0.0051304
	speed: 0.0400s/iter; left time: 500.5272s
Epoch: 15 cost time: 14.231785774230957
Epoch: 15, Steps: 356 | Train Loss: 0.0070315 Vali Loss: 0.0920275 Test Loss: 0.0923247
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.103515625e-09
	iters: 100, epoch: 16 | loss: 0.0055082
	speed: 0.1016s/iter; left time: 1256.0978s
	iters: 200, epoch: 16 | loss: 0.0058639
	speed: 0.0399s/iter; left time: 489.6640s
	iters: 300, epoch: 16 | loss: 0.0148671
	speed: 0.0400s/iter; left time: 486.2629s
Epoch: 16 cost time: 14.215492010116577
Epoch: 16, Steps: 356 | Train Loss: 0.0070883 Vali Loss: 0.0921325 Test Loss: 0.0918150
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (131, 32, 24, 1) (131, 32, 24, 1)
test shape: (4192, 24, 1) (4192, 24, 1)
mse:0.09131345897912979, mae:0.10877463966608047
>>>>>>>predicting : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0189004
	speed: 0.0560s/iter; left time: 990.4643s
	iters: 200, epoch: 1 | loss: 0.0542586
	speed: 0.0394s/iter; left time: 693.2569s
	iters: 300, epoch: 1 | loss: 0.0359084
	speed: 0.0394s/iter; left time: 690.0844s
Epoch: 1 cost time: 15.258922338485718
Epoch: 1, Steps: 356 | Train Loss: 0.1195342 Vali Loss: 0.1374229 Test Loss: 0.1367013
Validation loss decreased (inf --> 0.137423).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0109053
	speed: 0.1018s/iter; left time: 1765.9623s
	iters: 200, epoch: 2 | loss: 0.0077334
	speed: 0.0396s/iter; left time: 682.2712s
	iters: 300, epoch: 2 | loss: 0.0407069
	speed: 0.0396s/iter; left time: 678.6618s
Epoch: 2 cost time: 14.086203575134277
Epoch: 2, Steps: 356 | Train Loss: 0.0197516 Vali Loss: 0.1334409 Test Loss: 0.1333032
Validation loss decreased (0.137423 --> 0.133441).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0089297
	speed: 0.1030s/iter; left time: 1749.1361s
	iters: 200, epoch: 3 | loss: 0.0098273
	speed: 0.0396s/iter; left time: 668.0979s
	iters: 300, epoch: 3 | loss: 0.0375181
	speed: 0.0396s/iter; left time: 664.8454s
Epoch: 3 cost time: 14.08323049545288
Epoch: 3, Steps: 356 | Train Loss: 0.0137051 Vali Loss: 0.1318159 Test Loss: 0.1321315
Validation loss decreased (0.133441 --> 0.131816).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0408546
	speed: 0.1031s/iter; left time: 1714.2831s
	iters: 200, epoch: 4 | loss: 0.0055944
	speed: 0.0397s/iter; left time: 655.5927s
	iters: 300, epoch: 4 | loss: 0.0040547
	speed: 0.0396s/iter; left time: 650.9214s
Epoch: 4 cost time: 14.106844425201416
Epoch: 4, Steps: 356 | Train Loss: 0.0107247 Vali Loss: 0.1317775 Test Loss: 0.1326119
Validation loss decreased (0.131816 --> 0.131778).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0047388
	speed: 0.1046s/iter; left time: 1702.4979s
	iters: 200, epoch: 5 | loss: 0.0042745
	speed: 0.0397s/iter; left time: 642.1231s
	iters: 300, epoch: 5 | loss: 0.0145586
	speed: 0.0397s/iter; left time: 637.7996s
Epoch: 5 cost time: 14.126079320907593
Epoch: 5, Steps: 356 | Train Loss: 0.0098705 Vali Loss: 0.1347678 Test Loss: 0.1349638
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0052227
	speed: 0.1005s/iter; left time: 1599.5976s
	iters: 200, epoch: 6 | loss: 0.0051734
	speed: 0.0397s/iter; left time: 628.5243s
	iters: 300, epoch: 6 | loss: 0.0050177
	speed: 0.0398s/iter; left time: 625.3482s
Epoch: 6 cost time: 14.137699842453003
Epoch: 6, Steps: 356 | Train Loss: 0.0091074 Vali Loss: 0.1343281 Test Loss: 0.1327729
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0037764
	speed: 0.1005s/iter; left time: 1563.9510s
	iters: 200, epoch: 7 | loss: 0.0054815
	speed: 0.0397s/iter; left time: 614.1722s
	iters: 300, epoch: 7 | loss: 0.0036717
	speed: 0.0398s/iter; left time: 610.8494s
Epoch: 7 cost time: 14.14723014831543
Epoch: 7, Steps: 356 | Train Loss: 0.0087454 Vali Loss: 0.1346936 Test Loss: 0.1343875
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0226872
	speed: 0.1006s/iter; left time: 1529.8912s
	iters: 200, epoch: 8 | loss: 0.0197828
	speed: 0.0398s/iter; left time: 601.3054s
	iters: 300, epoch: 8 | loss: 0.0045011
	speed: 0.0398s/iter; left time: 597.2582s
Epoch: 8 cost time: 14.161950588226318
Epoch: 8, Steps: 356 | Train Loss: 0.0086257 Vali Loss: 0.1345783 Test Loss: 0.1361986
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0245923
	speed: 0.1006s/iter; left time: 1494.4793s
	iters: 200, epoch: 9 | loss: 0.0033674
	speed: 0.0397s/iter; left time: 586.2127s
	iters: 300, epoch: 9 | loss: 0.0035485
	speed: 0.0398s/iter; left time: 582.9273s
Epoch: 9 cost time: 14.151770114898682
Epoch: 9, Steps: 356 | Train Loss: 0.0085964 Vali Loss: 0.1355489 Test Loss: 0.1351004
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (131, 32, 24, 1) (131, 32, 24, 1)
test shape: (4192, 24, 1) (4192, 24, 1)
mse:0.13232538104057312, mae:0.11801981925964355
>>>>>>>predicting : informer_sl96_ll48_pl24_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0650380
	speed: 0.0989s/iter; left time: 1720.4755s
	iters: 200, epoch: 1 | loss: 0.0491261
	speed: 0.0832s/iter; left time: 1438.6883s
	iters: 300, epoch: 1 | loss: 0.0977557
	speed: 0.0832s/iter; left time: 1430.9522s
Epoch: 1 cost time: 30.305811166763306
Epoch: 1, Steps: 350 | Train Loss: 0.1134258 Vali Loss: 0.2321849 Test Loss: 0.2331962
Validation loss decreased (inf --> 0.232185).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0282198
	speed: 0.2118s/iter; left time: 3611.3508s
	iters: 200, epoch: 2 | loss: 0.0255784
	speed: 0.0834s/iter; left time: 1413.9745s
	iters: 300, epoch: 2 | loss: 0.0154333
	speed: 0.0834s/iter; left time: 1406.0608s
Epoch: 2 cost time: 29.19623613357544
Epoch: 2, Steps: 350 | Train Loss: 0.0297760 Vali Loss: 0.2187566 Test Loss: 0.2204999
Validation loss decreased (0.232185 --> 0.218757).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0259531
	speed: 0.2147s/iter; left time: 3586.4333s
	iters: 200, epoch: 3 | loss: 0.0077991
	speed: 0.0837s/iter; left time: 1390.0143s
	iters: 300, epoch: 3 | loss: 0.0092123
	speed: 0.0839s/iter; left time: 1383.6731s
Epoch: 3 cost time: 29.317435264587402
Epoch: 3, Steps: 350 | Train Loss: 0.0145925 Vali Loss: 0.2312812 Test Loss: 0.2318687
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0157569
	speed: 0.2109s/iter; left time: 3447.7553s
	iters: 200, epoch: 4 | loss: 0.0070094
	speed: 0.0839s/iter; left time: 1363.6351s
	iters: 300, epoch: 4 | loss: 0.0139164
	speed: 0.0839s/iter; left time: 1354.3006s
Epoch: 4 cost time: 29.342977046966553
Epoch: 4, Steps: 350 | Train Loss: 0.0109954 Vali Loss: 0.2236047 Test Loss: 0.2237091
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0127855
	speed: 0.2109s/iter; left time: 3373.8960s
	iters: 200, epoch: 5 | loss: 0.0059954
	speed: 0.0838s/iter; left time: 1331.8176s
	iters: 300, epoch: 5 | loss: 0.0135203
	speed: 0.0838s/iter; left time: 1324.4860s
Epoch: 5 cost time: 29.31762719154358
Epoch: 5, Steps: 350 | Train Loss: 0.0090862 Vali Loss: 0.2127267 Test Loss: 0.2143998
Validation loss decreased (0.218757 --> 0.212727).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0101460
	speed: 0.2160s/iter; left time: 3380.4631s
	iters: 200, epoch: 6 | loss: 0.0078616
	speed: 0.0835s/iter; left time: 1299.1076s
	iters: 300, epoch: 6 | loss: 0.0104092
	speed: 0.0837s/iter; left time: 1292.9618s
Epoch: 6 cost time: 29.292522192001343
Epoch: 6, Steps: 350 | Train Loss: 0.0081590 Vali Loss: 0.2155208 Test Loss: 0.2161869
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0062131
	speed: 0.2112s/iter; left time: 3231.9868s
	iters: 200, epoch: 7 | loss: 0.0113593
	speed: 0.0839s/iter; left time: 1275.5606s
	iters: 300, epoch: 7 | loss: 0.0093030
	speed: 0.0840s/iter; left time: 1267.7691s
Epoch: 7 cost time: 29.368062019348145
Epoch: 7, Steps: 350 | Train Loss: 0.0077320 Vali Loss: 0.2150865 Test Loss: 0.2166356
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0083675
	speed: 0.2110s/iter; left time: 3155.0136s
	iters: 200, epoch: 8 | loss: 0.0049660
	speed: 0.0839s/iter; left time: 1245.5903s
	iters: 300, epoch: 8 | loss: 0.0097029
	speed: 0.0838s/iter; left time: 1235.7533s
Epoch: 8 cost time: 29.33662724494934
Epoch: 8, Steps: 350 | Train Loss: 0.0074797 Vali Loss: 0.2127075 Test Loss: 0.2149283
Validation loss decreased (0.212727 --> 0.212707).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0047277
	speed: 0.2141s/iter; left time: 3125.4451s
	iters: 200, epoch: 9 | loss: 0.0068586
	speed: 0.0838s/iter; left time: 1215.0069s
	iters: 300, epoch: 9 | loss: 0.0042957
	speed: 0.0839s/iter; left time: 1207.8199s
Epoch: 9 cost time: 29.3381028175354
Epoch: 9, Steps: 350 | Train Loss: 0.0073675 Vali Loss: 0.2157447 Test Loss: 0.2172680
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0073969
	speed: 0.2111s/iter; left time: 3008.9858s
	iters: 200, epoch: 10 | loss: 0.0048437
	speed: 0.0838s/iter; left time: 1185.8526s
	iters: 300, epoch: 10 | loss: 0.0061707
	speed: 0.0837s/iter; left time: 1175.8242s
Epoch: 10 cost time: 29.31560230255127
Epoch: 10, Steps: 350 | Train Loss: 0.0073155 Vali Loss: 0.2166948 Test Loss: 0.2173310
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0057909
	speed: 0.2108s/iter; left time: 2930.0092s
	iters: 200, epoch: 11 | loss: 0.0067566
	speed: 0.0839s/iter; left time: 1158.5466s
	iters: 300, epoch: 11 | loss: 0.0043546
	speed: 0.0838s/iter; left time: 1148.0019s
Epoch: 11 cost time: 29.332483530044556
Epoch: 11, Steps: 350 | Train Loss: 0.0072084 Vali Loss: 0.2158415 Test Loss: 0.2173941
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.0049911
	speed: 0.2111s/iter; left time: 2860.6024s
	iters: 200, epoch: 12 | loss: 0.0069283
	speed: 0.0838s/iter; left time: 1127.6091s
	iters: 300, epoch: 12 | loss: 0.0058532
	speed: 0.0838s/iter; left time: 1118.7522s
Epoch: 12 cost time: 29.33986258506775
Epoch: 12, Steps: 350 | Train Loss: 0.0072900 Vali Loss: 0.2164858 Test Loss: 0.2169242
EarlyStopping counter: 4 out of 5
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.0061850
	speed: 0.2113s/iter; left time: 2788.9711s
	iters: 200, epoch: 13 | loss: 0.0072152
	speed: 0.0839s/iter; left time: 1098.9222s
	iters: 300, epoch: 13 | loss: 0.0115198
	speed: 0.0840s/iter; left time: 1091.9413s
Epoch: 13 cost time: 29.377418041229248
Epoch: 13, Steps: 350 | Train Loss: 0.0072311 Vali Loss: 0.2145571 Test Loss: 0.2175444
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (125, 32, 60, 1) (125, 32, 60, 1)
test shape: (4000, 60, 1) (4000, 60, 1)
mse:0.21468737721443176, mae:0.2043539434671402
>>>>>>>predicting : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start training : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
	iters: 100, epoch: 1 | loss: 0.0706682
	speed: 0.0983s/iter; left time: 1710.4471s
	iters: 200, epoch: 1 | loss: 0.0281337
	speed: 0.0824s/iter; left time: 1425.1562s
	iters: 300, epoch: 1 | loss: 0.0272520
	speed: 0.0824s/iter; left time: 1418.2039s
Epoch: 1 cost time: 30.03724956512451
Epoch: 1, Steps: 350 | Train Loss: 0.1392364 Vali Loss: 0.2025637 Test Loss: 0.2049357
Validation loss decreased (inf --> 0.202564).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.0168027
	speed: 0.2076s/iter; left time: 3539.5942s
	iters: 200, epoch: 2 | loss: 0.0208212
	speed: 0.0827s/iter; left time: 1401.2765s
	iters: 300, epoch: 2 | loss: 0.0174883
	speed: 0.0827s/iter; left time: 1392.9951s
Epoch: 2 cost time: 28.93540859222412
Epoch: 2, Steps: 350 | Train Loss: 0.0271924 Vali Loss: 0.1934146 Test Loss: 0.1953866
Validation loss decreased (0.202564 --> 0.193415).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.0182140
	speed: 0.2092s/iter; left time: 3493.4072s
	iters: 200, epoch: 3 | loss: 0.0148548
	speed: 0.0828s/iter; left time: 1375.3000s
	iters: 300, epoch: 3 | loss: 0.0093337
	speed: 0.0829s/iter; left time: 1368.4603s
Epoch: 3 cost time: 28.997881412506104
Epoch: 3, Steps: 350 | Train Loss: 0.0186326 Vali Loss: 0.1858171 Test Loss: 0.1872408
Validation loss decreased (0.193415 --> 0.185817).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.0077444
	speed: 0.2093s/iter; left time: 3422.2715s
	iters: 200, epoch: 4 | loss: 0.0536892
	speed: 0.0829s/iter; left time: 1347.8709s
	iters: 300, epoch: 4 | loss: 0.0174363
	speed: 0.0829s/iter; left time: 1338.8211s
Epoch: 4 cost time: 29.012589931488037
Epoch: 4, Steps: 350 | Train Loss: 0.0139142 Vali Loss: 0.1833088 Test Loss: 0.1845674
Validation loss decreased (0.185817 --> 0.183309).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.0226370
	speed: 0.2118s/iter; left time: 3388.4687s
	iters: 200, epoch: 5 | loss: 0.0081631
	speed: 0.0830s/iter; left time: 1320.2142s
	iters: 300, epoch: 5 | loss: 0.0156629
	speed: 0.0829s/iter; left time: 1309.1154s
Epoch: 5 cost time: 29.025020837783813
Epoch: 5, Steps: 350 | Train Loss: 0.0122545 Vali Loss: 0.1820499 Test Loss: 0.1820224
Validation loss decreased (0.183309 --> 0.182050).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.0105109
	speed: 0.2118s/iter; left time: 3314.7126s
	iters: 200, epoch: 6 | loss: 0.0103779
	speed: 0.0830s/iter; left time: 1291.2603s
	iters: 300, epoch: 6 | loss: 0.0052362
	speed: 0.0831s/iter; left time: 1284.3183s
Epoch: 6 cost time: 29.064249753952026
Epoch: 6, Steps: 350 | Train Loss: 0.0113620 Vali Loss: 0.1821553 Test Loss: 0.1839319
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.0073131
	speed: 0.2065s/iter; left time: 3159.4244s
	iters: 200, epoch: 7 | loss: 0.0047564
	speed: 0.0830s/iter; left time: 1261.3201s
	iters: 300, epoch: 7 | loss: 0.0151880
	speed: 0.0830s/iter; left time: 1253.1820s
Epoch: 7 cost time: 29.029605627059937
Epoch: 7, Steps: 350 | Train Loss: 0.0107340 Vali Loss: 0.1786942 Test Loss: 0.1801449
Validation loss decreased (0.182050 --> 0.178694).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.0151619
	speed: 0.2110s/iter; left time: 3154.8981s
	iters: 200, epoch: 8 | loss: 0.0230881
	speed: 0.0830s/iter; left time: 1233.2861s
	iters: 300, epoch: 8 | loss: 0.0178158
	speed: 0.0830s/iter; left time: 1224.9077s
Epoch: 8 cost time: 29.049718856811523
Epoch: 8, Steps: 350 | Train Loss: 0.0105931 Vali Loss: 0.1791769 Test Loss: 0.1797802
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.0054974
	speed: 0.2068s/iter; left time: 3019.8019s
	iters: 200, epoch: 9 | loss: 0.0149360
	speed: 0.0832s/iter; left time: 1206.3454s
	iters: 300, epoch: 9 | loss: 0.0089399
	speed: 0.0830s/iter; left time: 1195.8026s
Epoch: 9 cost time: 29.082422256469727
Epoch: 9, Steps: 350 | Train Loss: 0.0102699 Vali Loss: 0.1766389 Test Loss: 0.1791123
Validation loss decreased (0.178694 --> 0.176639).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.0038932
	speed: 0.2106s/iter; left time: 3001.1797s
	iters: 200, epoch: 10 | loss: 0.0078022
	speed: 0.0827s/iter; left time: 1170.9825s
	iters: 300, epoch: 10 | loss: 0.0079333
	speed: 0.0828s/iter; left time: 1163.1358s
Epoch: 10 cost time: 28.963141679763794
Epoch: 10, Steps: 350 | Train Loss: 0.0103976 Vali Loss: 0.1779069 Test Loss: 0.1786650
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.0064855
	speed: 0.2063s/iter; left time: 2867.9703s
	iters: 200, epoch: 11 | loss: 0.0061870
	speed: 0.0831s/iter; left time: 1146.6882s
	iters: 300, epoch: 11 | loss: 0.0063817
	speed: 0.0829s/iter; left time: 1136.3871s
Epoch: 11 cost time: 29.0306613445282
Epoch: 11, Steps: 350 | Train Loss: 0.0102208 Vali Loss: 0.1780321 Test Loss: 0.1786596
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.0088988
	speed: 0.2066s/iter; left time: 2799.4930s
	iters: 200, epoch: 12 | loss: 0.0047677
	speed: 0.0830s/iter; left time: 1116.5936s
	iters: 300, epoch: 12 | loss: 0.0228340
	speed: 0.0829s/iter; left time: 1107.1147s
Epoch: 12 cost time: 29.036954164505005
Epoch: 12, Steps: 350 | Train Loss: 0.0102584 Vali Loss: 0.1793421 Test Loss: 0.1801245
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.0132647
	speed: 0.2068s/iter; left time: 2730.6262s
	iters: 200, epoch: 13 | loss: 0.0082472
	speed: 0.0831s/iter; left time: 1089.2517s
	iters: 300, epoch: 13 | loss: 0.0147865
	speed: 0.0831s/iter; left time: 1080.7409s
Epoch: 13 cost time: 29.1012020111084
Epoch: 13, Steps: 350 | Train Loss: 0.0102335 Vali Loss: 0.1779551 Test Loss: 0.1785617
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-08
	iters: 100, epoch: 14 | loss: 0.0051501
	speed: 0.2068s/iter; left time: 2657.3430s
	iters: 200, epoch: 14 | loss: 0.0068552
	speed: 0.0830s/iter; left time: 1058.9578s
	iters: 300, epoch: 14 | loss: 0.0065552
	speed: 0.0830s/iter; left time: 1050.2250s
Epoch: 14 cost time: 29.0527081489563
Epoch: 14, Steps: 350 | Train Loss: 0.0102290 Vali Loss: 0.1743978 Test Loss: 0.1793883
Validation loss decreased (0.176639 --> 0.174398).  Saving model ...
Updating learning rate to 1.220703125e-08
	iters: 100, epoch: 15 | loss: 0.0264000
	speed: 0.2109s/iter; left time: 2637.0223s
	iters: 200, epoch: 15 | loss: 0.0094327
	speed: 0.0830s/iter; left time: 1029.4861s
	iters: 300, epoch: 15 | loss: 0.0050777
	speed: 0.0830s/iter; left time: 1021.1899s
Epoch: 15 cost time: 29.05374503135681
Epoch: 15, Steps: 350 | Train Loss: 0.0102845 Vali Loss: 0.1784491 Test Loss: 0.1799253
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.103515625e-09
	iters: 100, epoch: 16 | loss: 0.0191565
	speed: 0.2065s/iter; left time: 2509.6051s
	iters: 200, epoch: 16 | loss: 0.0056900
	speed: 0.0829s/iter; left time: 999.0131s
	iters: 300, epoch: 16 | loss: 0.0058366
	speed: 0.0829s/iter; left time: 990.9859s
Epoch: 16 cost time: 29.02918577194214
Epoch: 16, Steps: 350 | Train Loss: 0.0102396 Vali Loss: 0.1780427 Test Loss: 0.1782228
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.0517578125e-09
	iters: 100, epoch: 17 | loss: 0.0046471
	speed: 0.2064s/iter; left time: 2436.1855s
	iters: 200, epoch: 17 | loss: 0.0062688
	speed: 0.0830s/iter; left time: 970.7985s
	iters: 300, epoch: 17 | loss: 0.0070880
	speed: 0.0830s/iter; left time: 962.6363s
Epoch: 17 cost time: 29.03105401992798
Epoch: 17, Steps: 350 | Train Loss: 0.0102783 Vali Loss: 0.1799193 Test Loss: 0.1794628
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.52587890625e-09
	iters: 100, epoch: 18 | loss: 0.0124341
	speed: 0.2066s/iter; left time: 2365.4573s
	iters: 200, epoch: 18 | loss: 0.0050356
	speed: 0.0830s/iter; left time: 941.9395s
	iters: 300, epoch: 18 | loss: 0.0060754
	speed: 0.0829s/iter; left time: 932.9638s
--microservice_id^[[BEpoch: 18 cost time: 29.029566764831543
Epoch: 18, Steps: 350 | Train Loss: 0.0102640 Vali Loss: 0.1769325 Test Loss: 0.1783029
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.62939453125e-10
	iters: 100, epoch: 19 | loss: 0.0126831
	speed: 0.2062s/iter; left time: 2288.8655s
	iters: 200, epoch: 19 | loss: 0.0111007
	speed: 0.0829s/iter; left time: 911.6691s
	iters: 300, epoch: 19 | loss: 0.0164307
	speed: 0.0830s/iter; left time: 904.7191s
Epoch: 19 cost time: 29.004631996154785
Epoch: 19, Steps: 350 | Train Loss: 0.0102582 Vali Loss: 0.1774485 Test Loss: 0.1786080
EarlyStopping counter: 5 out of 5
Early stopping
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>>>>>>>start testing : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue>>>>>>>>>>>>>>>>>>>>>>>>>>
test shape: (125, 32, 60, 1) (125, 32, 60, 1)
test shape: (4000, 60, 1) (4000, 60, 1)
mse:0.17762213945388794, mae:0.17029371857643127
>>>>>>>predicting : informer_sl240_ll120_pl60_dm1024_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
/its/home/ms2420/temporal_gnn_network_log_data_2/src/forecasting/workload_prediction.py:356: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(best_model_path))
>